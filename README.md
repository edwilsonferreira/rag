# Sistema de Pesquisa RAG com Suporte Duplo para LLMs (Gemini + Ollama)

**Nota Importante:** O c√≥digo-fonte base para o sistema descrito neste guia foi gerado com o valioso apoio da intelig√™ncia artificial Gemini, desenvolvida pelo Google. As funcionalidades e estruturas foram ent√£o iterativamente refinadas e adaptadas para os requisitos espec√≠ficos deste projeto de pesquisa.

## Vis√£o Geral do Projeto

Este projeto implementa um sistema de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) que permite aos usu√°rios fazer perguntas em linguagem natural sobre um conjunto de documentos PDF e Markdown. O sistema utiliza **ChromaDB** para **armazenamento persistente** de chunks de texto e seus embeddings, resultando em inicializa√ß√µes significativamente mais r√°pidas ap√≥s o primeiro processamento. 

**üÜï NOVIDADE:** O sistema agora oferece **suporte duplo para LLMs**, permitindo escolher entre:
- **Google Gemini** (via API - requer chave de API)
- **Ollama Local** (executado localmente - privacidade total)

As respostas s√£o geradas pelo LLM selecionado, com contexto relevante extra√≠do dos documentos. A interface do usu√°rio √© fornecida atrav√©s de uma aplica√ß√£o web Streamlit com detec√ß√£o autom√°tica do provedor ativo.

O objetivo √© fornecer uma ferramenta de pesquisa sem√¢ntica poderosa e flex√≠vel que possa ser executada localmente, garantindo a privacidade dos dados e permitindo a customiza√ß√£o dos modelos utilizados.

## üöÄ Funcionalidades Principais

### üß† **Suporte Duplo para LLMs (NOVO!)**
* **Google Gemini:** Integra√ß√£o via API (modelo gemini-2.5-flash)
* **Ollama Local:** Para m√°xima privacidade (modelos llama3.2:1b, llama3:latest, etc.)
* **Switching Din√¢mico:** Alternar entre provedores via `config.py`
* **Interface Adaptativa:** Detec√ß√£o autom√°tica do provedor ativo

### üõ°Ô∏è **Seguran√ßa Integrada (MELHORADO!)**
* **Diretiva de Seguran√ßa Flex√≠vel:** Prote√ß√£o contra engenharia social mantendo funcionalidade
* **Instru√ß√µes Protegidas:** Sistema imune a tentativas de vazamento de configura√ß√µes internas
* **Resposta Inteligente:** Permite explica√ß√µes detalhadas de procedimentos documentados

### üåê **Sistema de Conhecimento Externo (NOVO!)**
* **Wikipedia Integration:** Complementa informa√ß√µes locais com conhecimento educacional
* **Base Conceitual:** Defini√ß√µes autom√°ticas para termos t√©cnicos n√£o cobertos
* **Ativa√ß√£o Inteligente:** Usa conhecimento externo apenas quando informa√ß√µes locais s√£o insuficientes
* **Controle Granular:** Configura√ß√£o detalhada via `EXTERNAL_KNOWLEDGE_CONFIG`

### üìä **Sistema de Logging Aprimorado (NOVO!)**
* **M√©tricas de Qualidade:** An√°lise autom√°tica da relev√¢ncia dos chunks recuperados
* **Avisos Inteligentes:** Alertas quando relev√¢ncia est√° baixa com sugest√µes de melhoria
* **Rastreamento Completo:** Log detalhado do fluxo de processamento para debugging
* **Fallback Estruturado:** Respostas organizadas quando informa√ß√µes est√£o incompletas

### üìö **Processamento de Documentos**
* **M√∫ltiplos Formatos (NOVO!):** Suporte a arquivos PDF (.pdf), Markdown (.md, .markdown)
* **Processamento de PDFs:** Extrai texto e tabelas de arquivos PDF com PyMuPDF
* **Processamento de Markdown:** Suporte nativo a arquivos .md e .markdown com encoding UTF-8
* **Persist√™ncia com ChromaDB:** Chunks de texto e seus embeddings s√£o armazenados no ChromaDB, evitando reprocessamento
* **Processamento Inteligente:** Detec√ß√£o autom√°tica de documentos novos/modificados com atualiza√ß√£o incremental
* **Gera√ß√£o de Embeddings:** Utiliza modelos `SentenceTransformers` (all-MiniLM-L6-v2)
* **Busca Vetorial Eficiente:** ChromaDB gerencia indexa√ß√£o e busca sem√¢ntica unificada

### üíª **Interfaces M√∫ltiplas**
* **Interface Web Interativa:** Streamlit com sidebar informativo
* **Interface de Terminal:** Script `rag_terminal.py` para uso via CLI
* **Consultas em Lote:** Script `rag_batch_query.py` para processamento automatizado
* **Configura√ß√£o Centralizada:** Via `src/rag_app/config.py` e arquivo `.env`
* **Timestamps no Chat:** Opcional para tracking de conversas

## üõ†Ô∏è Tecnologias Utilizadas

### üß† **Modelos de Linguagem**
* **Google Gemini:** API do Google para LLM em nuvem (gemini-2.5-flash)
* **Ollama:** Para servir LLMs localmente (llama3.2:1b, llama3:latest, etc.)

### üîß **Framework e Interface**
* **Python:** Linguagem de programa√ß√£o principal (vers√£o 3.9+ recomendada)
* **Streamlit:** Para a interface web interativa
* **python-dotenv:** Para gerenciamento seguro de vari√°veis de ambiente

### üóÑÔ∏è **Armazenamento e Processamento**
* **ChromaDB:** Para armazenamento persistente e busca de embeddings
* **SentenceTransformers:** Para gera√ß√£o de embeddings de texto (all-MiniLM-L6-v2)
* **PyMuPDF (Fitz):** Para extra√ß√£o de texto e tabelas de PDFs
* **Processamento de Markdown:** Suporte nativo com encoding UTF-8 e fallback latin-1
* **NumPy:** Para opera√ß√µes num√©ricas e manipula√ß√£o de arrays

## üèóÔ∏è Arquitetura do Sistema

O diagrama abaixo ilustra os principais componentes do sistema RAG, suas intera√ß√µes e o fluxo de dados, desde o processamento inicial dos documentos PDF at√© a gera√ß√£o da resposta para a consulta do usu√°rio. Ele destaca como as entradas s√£o processadas, onde os dados s√£o armazenados (ChromaDB), e como os diferentes scripts e modelos interagem.

![Diagrama do Sistema RAG](assets/diagrama_rag_sistema.svg)

## üìÇ Estrutura do Projeto

A estrutura de diret√≥rios e arquivos esperada para o projeto √©:

seu_projeto_rag/  
‚îú‚îÄ‚îÄ src/  
‚îÇ   ‚îî‚îÄ‚îÄ rag_app/             # Pacote Python principal  
‚îÇ       ‚îú‚îÄ‚îÄ init.py  
‚îÇ       ‚îú‚îÄ‚îÄ config.py  
‚îÇ       ‚îú‚îÄ‚îÄ rag_core.py          # Core melhorado com conhecimento externo
‚îÇ       ‚îú‚îÄ‚îÄ external_knowledge.py # Sistema de conhecimento externo (NOVO!)
‚îÇ       ‚îú‚îÄ‚îÄ rag_web.py  
‚îÇ       ‚îú‚îÄ‚îÄ rag_terminal.py  
‚îÇ       ‚îî‚îÄ‚îÄ rag_batch_query.py  
‚îú‚îÄ‚îÄ data/                     # Documentos de entrada: PDFs (.pdf) e Markdown (.md, .markdown)  
‚îú‚îÄ‚îÄ chroma_db_store/          # Banco de dados ChromaDB (relativo √† raiz do projeto)  
‚îú‚îÄ‚îÄ assets/                   # Ativos como diagramas  
‚îÇ   ‚îî‚îÄ‚îÄ diagrama_rag_sistema.svg  
‚îú‚îÄ‚îÄ processed_files_status.json # Rastreia PDFs processados (relativo √† raiz do projeto)  
‚îú‚îÄ‚îÄ requirements.txt          # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env                      # Vari√°veis de ambiente (chaves de API)
‚îú‚îÄ‚îÄ DIRETIVA_SEGURANCA.md    # Documenta√ß√£o da diretiva de seguran√ßa
‚îî‚îÄ‚îÄ README.md                 # Este arquivo  
  

## üîß Configura√ß√£o de Provedor LLM

O sistema suporta dois provedores de LLM que podem ser alternados via configura√ß√£o:

### üåê **Google Gemini (Recomendado para Produ√ß√£o)**
```python
# Em src/rag_app/config.py
LLM_PROVIDER: str = "gemini"
```

**Configura√ß√£o necess√°ria:**
1. Obter chave de API: https://makersuite.google.com/app/apikey
2. Criar arquivo `.env` na raiz do projeto:
   ```
   GOOGLE_API_KEY=sua_chave_aqui
   ```
3. **Vantagens:** Alta qualidade, sem setup local, sempre dispon√≠vel
4. **Desvantagens:** Requer internet, custo por uso, dados enviados para Google

### üè† **Ollama Local (Recomendado para Privacidade)**
```python
# Em src/rag_app/config.py
LLM_PROVIDER: str = "ollama"
```

**Configura√ß√£o necess√°ria:**
1. Instalar Ollama: https://ollama.com
2. Baixar modelo: `ollama pull llama3.2:1b` (recomendado para sistemas com pouca RAM)
3. **Vantagens:** Privacidade total, sem custos, funciona offline
4. **Desvantagens:** Requer hardware local, setup mais complexo

### üîÑ **Alternando entre Provedores**
Basta modificar `LLM_PROVIDER` em `config.py` e reiniciar a aplica√ß√£o. A interface detecta automaticamente o provedor ativo.

## ‚öôÔ∏è Configura√ß√£o e Execu√ß√£o do Sistema

Siga estes passos detalhados para configurar e executar o projeto.

### 1. Requisitos do Sistema
(Conforme detalhado anteriormente: Python 3.10/3.11, Ollama, Pip, Git, Hardware adequado)

### 2. Prepara√ß√£o do Ambiente

1.  **Instalar Python.**
2.  **Clonar o Reposit√≥rio (se aplic√°vel).**
3.  **Criar e Ativar um Ambiente Virtual:**
    Na pasta raiz do projeto (`seu_projeto_rag/`):
    ```bash
    python -m venv .venv
    ```
    Para ativar:
    * **Linux/macOS:** `source .venv/bin/activate`
    * **Windows (CMD):** `.venv\Scripts\activate.bat`
    * **Windows (PowerShell):** `.venv\Scripts\Activate.ps1`

### 3. Configura√ß√£o do Ollama
(Instalar Ollama, baixar modelos LLM como `ollama pull llama3:latest`).

### 4. Instala√ß√£o das Depend√™ncias Python
Com o ambiente virtual ativo, na pasta raiz do projeto:
```bash
pip install -r requirements.txt
```
(O arquivo requirements.txt deve listar pymupdf, sentence-transformers, ollama, numpy, streamlit, chromadb).

### 5. Configura√ß√£o do Projeto (src/rag_app/config.py)

O arquivo src/rag_app/config.py √© o centro de controle para os par√¢metros do sistema. Ajuste-o conforme suas necessidades. Os caminhos como DEFAULT_DATA_FOLDER, CHROMA_DB_PATH, PROCESSED_FILES_STATUS_JSON s√£o relativos ao diret√≥rio de onde os scripts s√£o executados (geralmente a raiz do projeto ao usar os comandos de execu√ß√£o recomendados).

Python
#### 5.1 src/rag_app/config.py (principais par√¢metros)
from typing import List # Necess√°rio para EXIT_COMMANDS

#### 5.2 Flags de Depura√ß√£o e Comportamento
PRINT_DEBUG_CHUNKS: bool = True       # True para imprimir chunks recuperados no console.
SHOW_CHAT_TIMESTAMPS: bool = True     # True para exibir data/hora no chat.
ALWAYS_INCLUDE_PAGE_IN_ANSWER: bool = False # True para instruir o LLM a SEMPRE tentar citar a fonte/p√°gina.

#### 5.3 Comandos para finalizar loops interativos (ex: rag_terminal)
EXIT_COMMANDS: List[str] = [
    "sair", "saia", "exit", "quit", "q",
    "parar", "pare", "stop",
    "finalizar", "finalize", "fim", "end",
    "fechar", "close",
    "terminar", "terminate", "bye"
]

#### 5.4 Modelos Padr√£o
DEFAULT_OLLAMA_MODEL: str = "llama3:latest"
DEFAULT_EMBEDDING_MODEL: str = "all-MiniLM-L6-v2"

#### 5.5 Caminhos e Nomes de Cole√ß√£o/Status
DEFAULT_DATA_FOLDER: str = "data"
CHROMA_DB_PATH: str = "./chroma_db_store" 
CHROMA_COLLECTION_NAME: str = "rag_documents"
PROCESSED_FILES_STATUS_JSON: str = "processed_files_status.json"

##### 5.6 Par√¢metros de Chunking e Recupera√ß√£o
DEFAULT_CHUNK_SIZE: int = 768
DEFAULT_CHUNK_OVERLAP: int = 100
DEFAULT_RETRIEVAL_K: int = 5

#### 5.7 Configura√ß√µes do Provedor LLM (NOVO!):

**LLM_PROVIDER: str = "gemini"**  
Define qual provedor usar: `"gemini"` (Google Gemini API) ou `"ollama"` (Ollama local)

**Configura√ß√µes do Google Gemini:**
- `GEMINI_MODEL: str = "gemini-2.5-flash"` - Modelo Gemini a usar
- `GOOGLE_API_KEY: str` - Carregado automaticamente do arquivo `.env`
- `GEMINI_TEMPERATURE: float = 0.7` - Criatividade das respostas (0.0-1.0)
- `GEMINI_MAX_TOKENS: int = 4096` - Limite de tokens por resposta

**Configura√ß√µes do Ollama:**
- `DEFAULT_OLLAMA_MODEL: str = "llama3.2:1b"` - Modelo Ollama (otimizado para baixa RAM)
- `OLLAMA_HOST: str = "http://192.168.64.4:11434"` - Endpoint do servidor Ollama

#### 5.8 Diretiva de Seguran√ßa (MELHORADO!):

**SECURITY_DIRECTIVE: str**  
Instru√ß√µes de seguran√ßa incorporadas automaticamente em todos os prompts para proteger contra:
- Tentativas de extra√ß√£o de instru√ß√µes internas
- Engenharia social e inje√ß√£o de prompts
- Vazamento de configura√ß√µes do sistema
- **NOVO:** Permite explica√ß√µes detalhadas de procedimentos e c√°lculos presentes nos documentos

#### 5.9 Sistema de Conhecimento Externo (NOVO!):

**ALLOW_EXTERNAL_KNOWLEDGE: bool = True**  
Controla se o sistema pode consultar fontes externas al√©m dos documentos carregados.

**EXTERNAL_KNOWLEDGE_CONFIG: dict**  
Configura√ß√µes detalhadas para uso de conhecimento externo:
- `min_chunks_threshold: int = 2` - M√≠nimo de chunks locais para N√ÉO usar externo
- `confidence_threshold: float = 0.3` - Score m√≠nimo para considerar chunks locais suficientes
- `max_external_results: int = 3` - M√°ximo de resultados externos a incluir
- `concept_keywords: list` - Palavras-chave que ativam busca por conceitos
- `enable_wikipedia: bool = True` - Ativa/desativa integra√ß√£o com Wikipedia
- `enable_educational_concepts: bool = True` - Ativa base de conceitos educacionais

#### 5.10 Sistema de Logging e Qualidade (NOVO!):

**M√©tricas Autom√°ticas:**
- An√°lise da dist√¢ncia m√©dia dos chunks recuperados
- Avisos quando relev√¢ncia est√° abaixo do limiar configurado
- Sugest√µes autom√°ticas para reformula√ß√£o de perguntas
- Log detalhado do fluxo de decis√£o do sistema

#### 5.11 Outros Par√¢metros:

**ALWAYS_INCLUDE_PAGE_IN_ANSWER: bool:**
Se True, instrui o LLM a sempre citar fonte e p√°gina nas respostas.

**EXIT_COMMANDS: List[str]:**
Comandos para encerrar loops interativos (terminal).


### 6. Preparando Dados de Entrada

Crie a pasta `data/` na raiz do projeto e adicione seus documentos nos formatos suportados:

#### üìÑ **Formatos de Documento Suportados**

**üÜï SUPORTE M√öLTIPLO:** O sistema agora processa automaticamente diferentes tipos de arquivo:

| Formato | Extens√µes | Caracter√≠sticas | Processamento |
|---------|-----------|-----------------|---------------|
| **PDF** | `.pdf` | Texto, tabelas, m√∫ltiplas p√°ginas | PyMuPDF (Fitz) - Extra√ß√£o completa |
| **Markdown** | `.md`, `.markdown` | Texto formatado, se√ß√µes, tabelas | Processamento nativo UTF-8 |

**Exemplo de estrutura:**

```text
data/
‚îú‚îÄ‚îÄ documento1.pdf        # ‚úÖ Ser√° processado
‚îú‚îÄ‚îÄ manual.md            # ‚úÖ Ser√° processado  
‚îú‚îÄ‚îÄ README.markdown      # ‚úÖ Ser√° processado
‚îú‚îÄ‚îÄ texto.txt           # ‚ùå N√£o suportado
‚îî‚îÄ‚îÄ arquivo.docx        # ‚ùå N√£o suportado
```

**üí° Exemplo de uso com Markdown:**

Suponha que voc√™ tenha um arquivo `data/guia.md`:

```markdown
# Guia do Sistema

## Configura√ß√£o Inicial
Para configurar o sistema, siga os seguintes passos:

1. Configure as vari√°veis de ambiente
2. Instale as depend√™ncias 
3. Execute o primeiro processamento

## Troubleshooting
- Erro X: Solu√ß√£o Y
- Erro Z: Solu√ß√£o W
```

**Resultado do processamento:**
- ‚úÖ Arquivo detectado automaticamente como Markdown
- ‚úÖ Conte√∫do processado preservando estrutura
- ‚úÖ Texto dividido em chunks sem√¢nticos
- ‚úÖ Embeddings gerados e armazenados no ChromaDB
- ‚úÖ Busca funcionando: "como configurar o sistema?" ‚Üí encontra se√ß√£o relevante

**Caracter√≠sticas do processamento:**
- **PDFs:** Extra√ß√£o de texto por p√°gina + tabelas quando dispon√≠veis
- **Markdown:** Leitura direta com preserva√ß√£o da formata√ß√£o
- **Detec√ß√£o autom√°tica:** Sistema identifica automaticamente o tipo de arquivo
- **Processamento incremental:** Apenas arquivos novos/modificados s√£o reprocessados
- **Encoding inteligente:** UTF-8 com fallback autom√°tico para latin-1

### 7. Executando os Componentes do Sistema

Importante: Todos os comandos a seguir devem ser executados a partir da pasta raiz do seu projeto (seu_projeto_rag/), com o ambiente virtual ativo e o servidor Ollama em execu√ß√£o.

### 7.1 Interface Web com Streamlit (rag_web.py) - Recomendado

Este script inicia a interface gr√°fica interativa no seu navegador com detec√ß√£o autom√°tica do provedor LLM ativo.

**Para Gemini:**
```bash
streamlit run src/rag_app/rag_web.py
```

**Para Ollama:**
```bash
streamlit run src/rag_app/rag_web.py --server.port 8502
```

**Funcionalidades da Interface:**
- üí¨ **Chat interativo** com hist√≥rico de conversas
- üîç **Sidebar informativo** mostra provedor LLM ativo, estat√≠sticas do ChromaDB
- üìä **M√©tricas em tempo real** (chunks indexados, modelo ativo, etc.)
- üîÑ **Detec√ß√£o autom√°tica** do provedor configurado (Gemini/Ollama)

Ap√≥s a execu√ß√£o, acesse o endere√ßo fornecido no terminal.

### 7.2 Interface de Terminal Interativa (rag_terminal.py)
Permite interagir com o sistema RAG diretamente pelo terminal.

```bash
python -m src.rag_app.rag_terminal
```   
 Siga as instru√ß√µes no terminal para fazer perguntas. Digite sair para encerrar.

Processamento de Perguntas em Lote (rag_batch_query.py):
Executa m√∫ltiplas perguntas de um arquivo de texto e opcionalmente salva as respostas.

```bash
python -m src.rag_app.rag_batch_query <ARQUIVO_DE_ENTRADA> -o <ARQUIVO_DE_SAIDA_OPCIONAL>
```  
<ARQUIVO_DE_ENTRADA>: Caminho para seu arquivo .txt com uma pergunta por linha.
<ARQUIVO_DE_SAIDA_OPCIONAL>: Caminho para um arquivo .txt onde as perguntas e respostas ser√£o salvas. Exemplo:
```bash
python -m src.rag_app.rag_batch_query data/lista_de_perguntas.txt -o resultados/respostas_em_lote.txt
```  
 Teste Direto do RAGCore (rag_core.py) - Para Desenvolvimento/Depura√ß√£o:
O arquivo rag_core.py cont√©m um bloco if __name__ == '__main__': que permite executar algumas consultas de teste predefinidas diretamente no console. Isso √© √∫til para verificar a l√≥gica central do RAG rapidamente.

```bash
python -m src.rag_app.rag_core
```  
 As perguntas de teste definidas dentro do rag_core.py ser√£o executadas, e as sa√≠das (incluindo chunks de depura√ß√£o, se PRINT_DEBUG_CHUNKS estiver True em config.py) ser√£o exibidas no console.

Comportamento de Inicializa√ß√£o (para todas as formas de execu√ß√£o):

Primeira Execu√ß√£o / Novos PDFs: O sistema processar√° os PDFs da pasta data/. Chunks e embeddings ser√£o gerados e salvos no diret√≥rio CHROMA_DB_PATH. O arquivo PROCESSED_FILES_STATUS_JSON ser√° criado/atualizado. Esta etapa inicial pode ser demorada.
Execu√ß√µes Subsequentes: O sistema se conectar√° ao ChromaDB existente e usar√° o PROCESSED_FILES_STATUS_JSON para verificar o estado dos arquivos. Apenas PDFs novos ou modificados ser√£o reprocessados. Isso torna a inicializa√ß√£o muito mais r√°pida.
### 8. Como Usar a Interface Web

(Conforme descrito anteriormente: interaja com o chat, consulte a barra lateral para informa√ß√µes do sistema).

## ÔøΩ Exemplos de Uso Pr√°tico

### Cen√°rio 1: Configura√ß√£o R√°pida com Google Gemini
```bash
# 1. Configurar chave de API
echo "GOOGLE_API_KEY=sua_chave_aqui" > .env

# 2. Configurar provedor em config.py
# LLM_PROVIDER: str = "gemini"

# 3. Executar interface web
streamlit run src/rag_app/rag_web.py
```

### Cen√°rio 2: Setup Local com Ollama (Apple Container)
```bash
# 1. Instalar e configurar Ollama
# Consulte: comandos_apple_container_ollama.txt

# 2. Configurar provedor em config.py  
# LLM_PROVIDER: str = "ollama"

# 3. Executar interface web
streamlit run src/rag_app/rag_web.py --server.port 8502
```

### Cen√°rio 3: Processamento em Lote
```bash
# 1. Criar arquivo de perguntas
echo "Qual √© a data da inscri√ß√£o?" > perguntas.txt
echo "Quais s√£o os requisitos?" >> perguntas.txt

# 2. Executar processamento
python -m src.rag_app.rag_batch_query perguntas.txt -o respostas.txt
```

### Cen√°rio 4: Teste do Sistema Melhorado (NOVO!)
```bash
# 1. Criar script de teste r√°pido
cat > test_pergunta.py << 'EOF'
import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
from rag_app.rag_core import RAGCore

rag = RAGCore()
pergunta = "Como calcular a renda per capita?"
print(f"Pergunta: {pergunta}")
print("="*50)
resposta = rag.answer_query(pergunta)
print(resposta)
EOF

# 2. Executar teste
python test_pergunta.py
```

**üîç O que o sistema faz automaticamente:**
- ‚úÖ Analisa qualidade dos chunks encontrados
- ‚úÖ Avisa se relev√¢ncia est√° baixa
- ‚úÖ Complementa com conhecimento externo quando necess√°rio
- ‚úÖ Fornece respostas estruturadas e detalhadas
- ‚úÖ Cita fontes e p√°ginas quando dispon√≠vel

## üìã Checklist de Configura√ß√£o

### ‚úÖ Para Google Gemini:
- [ ] Chave de API obtida em https://makersuite.google.com/app/apikey
- [ ] Arquivo `.env` criado com `GOOGLE_API_KEY`
- [ ] `LLM_PROVIDER = "gemini"` em `config.py`
- [ ] Conex√£o com internet dispon√≠vel

### ‚úÖ Para Ollama Local:
- [ ] Ollama instalado e executando
- [ ] Modelo baixado (`ollama pull llama3.2:1b`)
- [ ] `LLM_PROVIDER = "ollama"` em `config.py`  
- [ ] `OLLAMA_HOST` configurado corretamente
- [ ] Hardware adequado (m√≠nimo 2GB RAM)

### ‚úÖ Para Sistema de Conhecimento Externo:
- [ ] `ALLOW_EXTERNAL_KNOWLEDGE = True` em `config.py`
- [ ] Conex√£o com internet para Wikipedia (opcional)
- [ ] Configura√ß√£o de `EXTERNAL_KNOWLEDGE_CONFIG` ajustada
- [ ] Verificar logs para uso de conhecimento externo

### ‚úÖ Geral:
- [ ] Python 3.9+ instalado
- [ ] Depend√™ncias instaladas (`pip install -r requirements.txt`)
- [ ] Pasta `data/` criada com PDFs
- [ ] Ambiente virtual ativo

## üõ†Ô∏è Solu√ß√£o de Problemas Comuns

### üî¥ Erro: "Chave de API n√£o encontrada"
**Solu√ß√£o:** Verificar arquivo `.env` e vari√°vel `GOOGLE_API_KEY`
```bash
# Verificar se .env existe e tem conte√∫do
cat .env
# Deve mostrar: GOOGLE_API_KEY=sua_chave_aqui
```

### üî¥ Erro: "Ollama n√£o est√° executando"
**Solu√ß√£o:** Verificar status do container/servi√ßo
```bash
# Para Apple Container Runtime
container list
# Para instala√ß√£o nativa  
ollama list
```

### üî¥ Erro: "Modelo n√£o encontrado"
**Solu√ß√£o:** Baixar modelo necess√°rio
```bash
# Modelos recomendados por uso de RAM:
ollama pull llama3.2:1b    # ~1.3GB RAM
ollama pull llama3:latest  # ~4.6GB RAM
```

### üî¥ Interface web n√£o carrega
**Solu√ß√£o:** Verificar porta e configura√ß√µes
```bash
# Tentar porta alternativa
streamlit run src/rag_app/rag_web.py --server.port 8503

# Verificar logs do terminal para erros espec√≠ficos
```

### üî¥ ChromaDB inconsistente  
**Solu√ß√£o:** Reset completo do banco de dados
```bash
# ATEN√á√ÉO: Isso apaga todos os chunks processados
rm -rf chroma_db_store/
rm -f processed_files_status.json
# Reiniciar aplica√ß√£o para reprocessamento completo
```

üîÑ Manuten√ß√£o
(Conforme descrito anteriormente: backups do chroma_db_store/ e processed_files_status.json, reprocessamento, atualiza√ß√£o de modelos e depend√™ncias).
Lembre-se que ao remover um PDF da pasta data/, o sistema deve limpar seus dados do ChromaDB e do arquivo de status na pr√≥xima inicializa√ß√£o.

## üÜï Novidades desta Vers√£o

### v2.1 - Suporte a M√∫ltiplos Formatos de Documento

**üéØ Funcionalidades Principais Adicionadas:**

1. **üÜï Suporte a Arquivos Markdown**
   - Processamento nativo de arquivos `.md` e `.markdown`
   - Encoding UTF-8 com fallback autom√°tico para latin-1
   - Preserva√ß√£o da estrutura e formata√ß√£o do Markdown
   - Integra√ß√£o transparente com o sistema de busca sem√¢ntica

2. **üìÑ Arquitetura Multi-Formato**
   - Detec√ß√£o autom√°tica do tipo de arquivo (PDF, Markdown)
   - Processamento modular por tipo de documento
   - M√©todos espec√≠ficos: `_process_pdf_file()` e `_process_markdown_file()`
   - Sistema unificado de chunking e embeddings

3. **üîç Busca Sem√¢ntica Aprimorada**
   - Base de conhecimento unificada para PDFs e Markdown
   - Resultados de busca combinando ambos os formatos
   - Metadados preservados para identifica√ß√£o da fonte
   - Performance otimizada para m√∫ltiplos tipos de documento

### v2.0 - Suporte Duplo para LLMs + Seguran√ßa Avan√ßada

**üéØ Funcionalidades Anteriores:**

1. **Suporte Duplo para LLMs**
   - Google Gemini (API) + Ollama (Local)
   - Switching din√¢mico via configura√ß√£o
   - Interface adaptativa com detec√ß√£o autom√°tica

2. **Sistema de Seguran√ßa Integrado**
   - Diretiva de seguran√ßa autom√°tica em todos os prompts
   - Prote√ß√£o contra engenharia social e extra√ß√£o de instru√ß√µes
   - Sistema imune a tentativas de vazamento de configura√ß√µes

3. **Gerenciamento de Configura√ß√£o Avan√ßado**
   - Arquivo `.env` para chaves de API seguras
   - Configura√ß√£o centralizada em `config.py`
   - Suporte a m√∫ltiplos modelos e provedores

4. **Interface Aprimorada**
   - Sidebar informativo com estat√≠sticas em tempo real
   - Detec√ß√£o autom√°tica do provedor LLM ativo
   - Melhor experi√™ncia de usu√°rio

5. **Documenta√ß√£o Completa**
   - Guia de comandos para Apple Container Runtime
   - Troubleshooting detalhado para ambos provedores
   - Exemplos pr√°ticos de configura√ß√£o

**üîß Melhorias T√©cnicas v2.2:**
- ‚úÖ **Sistema de Conhecimento Externo:** Integra√ß√£o Wikipedia + base conceitual pr√≥pria
- ‚úÖ **Diretiva de Seguran√ßa Flex√≠vel:** Prote√ß√£o mantendo funcionalidade completa
- ‚úÖ **Logging Inteligente:** M√©tricas de qualidade e avisos autom√°ticos
- ‚úÖ **Sistema de Fallback:** Respostas estruturadas para informa√ß√µes incompletas
- ‚úÖ **Detec√ß√£o de Relev√¢ncia:** An√°lise autom√°tica da qualidade dos chunks
- ‚úÖ **Resposta Adaptativa:** Complementa conhecimento local com externo quando necess√°rio

**üîß Melhorias T√©cnicas v2.1:**
- ‚úÖ Arquitetura multi-formato com processamento modular
- ‚úÖ Sistema de detec√ß√£o autom√°tica de tipos de arquivo
- ‚úÖ M√©todos espec√≠ficos para cada formato de documento
- ‚úÖ Otimiza√ß√£o do chunking para diferentes tipos de conte√∫do
- ‚úÖ Base de conhecimento unificada com metadados preservados

**üîß Melhorias T√©cnicas v2.0:**
- ‚úÖ Refatora√ß√£o da arquitetura para suporte modular a LLMs
- ‚úÖ Implementa√ß√£o de padr√µes de seguran√ßa enterprise
- ‚úÖ Otimiza√ß√£o para modelos menores (llama3.2:1b para baixa RAM)
- ‚úÖ Melhoria na detec√ß√£o e tratamento de erros

## üÜï Novidades da Vers√£o 2.2

### üéØ **Problema Resolvido: Respostas Inadequadas**

**Situa√ß√£o Anterior:** O sistema recusava responder perguntas leg√≠timas sobre documentos:
```
‚ùå "Perd√£o, mas n√£o podemos discutir como calcular a renda bruta mensal"
```

**Situa√ß√£o Atual:** O sistema fornece respostas completas e √∫teis:
```
‚úÖ "Como voc√™ deseja saber como calcular a renda bruta mensal para verificar 
   se voc√™ se enquadra nas cotas de baixa renda, vou explicar passo a passo.
   
   Conforme o documento [Fonte: processed_document], para calcular a renda 
   bruta mensal, voc√™ precisa verificar se sua renda √© igual ou inferior 
   ao sal√°rio m√≠nimo per capita..."
```

### üîß **Implementa√ß√µes T√©cnicas**

#### **1. Sistema de Conhecimento Externo**
- **Arquivo:** `src/rag_app/external_knowledge.py` (NOVO)
- **Funcionalidade:** Complementa informa√ß√µes locais com Wikipedia e base conceitual
- **Ativa√ß√£o:** Autom√°tica quando chunks locais s√£o insuficientes
- **Exemplo de uso:** Defini√ß√£o de "renda per capita" quando n√£o expl√≠cita no documento

#### **2. Diretiva de Seguran√ßa Flex√≠vel**
- **Localiza√ß√£o:** `SECURITY_DIRECTIVE` em `config.py`
- **Melhoria:** Permite explica√ß√µes detalhadas mantendo prote√ß√£o
- **Resultado:** Sistema responde perguntas sobre procedimentos documentados

#### **3. Sistema de Qualidade e Logging**
- **Funcionalidade:** An√°lise autom√°tica da relev√¢ncia dos chunks
- **Avisos:** Alertas quando relev√¢ncia est√° baixa (dist√¢ncia > 0.8)
- **Sugest√µes:** Orienta√ß√µes autom√°ticas para reformular perguntas

#### **4. Fallback Estruturado**
- **Comportamento:** Respostas organizadas quando informa√ß√µes s√£o parciais
- **Inclui:** Sugest√µes pr√°ticas e orienta√ß√µes para o usu√°rio

### üìä **Exemplos Pr√°ticos de Melhorias**

#### **Antes vs Depois:**

| Pergunta | Resposta Anterior (v2.1) | Resposta Atual (v2.2) |
|----------|-------------------------|----------------------|
| "Como calcular renda?" | ‚ùå Recusa responder | ‚úÖ Explica passo a passo com base nos docs |
| "O que √© renda per capita?" | ‚ùå Informa√ß√£o limitada | ‚úÖ Defini√ß√£o completa + fonte do documento |
| "Quais documentos preciso?" | ‚úÖ Lista documentos | ‚úÖ Lista + explica como usar cada um |

#### **Logs de Qualidade:**
```
2025-09-30 11:40:43,394 - INFO - Recuperados 5 chunks via ChromaDB
2025-09-30 11:40:43,394 - INFO - Qualidade dos chunks: dist√¢ncia m√©dia=0.8842
2025-09-30 11:40:43,394 - WARNING - Chunks com baixa relev√¢ncia (dist. m√©dia: 0.8842)
                                    - considere reformular a pergunta
```

#### **Conhecimento Externo Ativo:**
```
2025-09-30 13:28:29,280 - INFO - Pergunta conceitual detectada: o que significa
2025-09-30 13:28:29,281 - INFO - Adicionando conhecimento externo para query: 
                                  'O que significa renda bruta mensal per capita?...'
```

## üí° Roadmap Futuro

### v2.3 (Em Planejamento)
- [ ] Suporte a mais formatos: DOCX, TXT, HTML, RTF
- [ ] Processamento de imagens em PDFs com OCR
- [ ] Suporte a tabelas complexas em Markdown
- [ ] Interface de configura√ß√£o via web para formatos
- [ ] Expans√£o do sistema de conhecimento externo (mais APIs)
- [ ] Machine Learning para otimiza√ß√£o autom√°tica de thresholds

### v2.4 (Planejado)
- [ ] Suporte a mais provedores LLM (Claude, OpenAI)
- [ ] Sistema de plugins para novos formatos
- [ ] Cache inteligente por tipo de documento
- [ ] API REST para integra√ß√£o externa
- [ ] Multi-tenancy e controle de acesso
- [ ] Deployment containerizado (Docker)

## ü§ù Contribui√ß√£o

Para contribuir com o projeto:

1. Fa√ßa um fork do reposit√≥rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commite suas mudan√ßas (`git commit -m 'Add nova feature'`)
4. Fa√ßa push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

## üìû Suporte

- **Issues:** Reporte bugs e solicite features via GitHub Issues
- **Documenta√ß√£o:** Consulte os arquivos `.md` na raiz do projeto
- **Configura√ß√£o:** Verifique `DIRETIVA_SEGURANCA.md` para detalhes de seguran√ßa

## üìÑ Licen√ßa

MIT License - A definir.  
