# Sistema de Pesquisa RAG com Ollama e Streamlit

Este projeto implementa um sistema de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) que permite aos usu√°rios fazer perguntas em linguagem natural sobre um conjunto de documentos PDF. As respostas s√£o geradas por um Modelo de Linguagem Grande (LLM) hospedado localmente via Ollama, com o contexto relevante extra√≠do dos documentos PDF. A interface do usu√°rio √© constru√≠da com Streamlit.

**Nota Importante:** O c√≥digo-fonte base para este sistema foi desenvolvido com o valioso apoio da intelig√™ncia artificial Gemini, desenvolvida pelo Google. As funcionalidades e estruturas foram ent√£o iterativamente refinadas e adaptadas para os requisitos espec√≠ficos deste projeto de pesquisa.

## üöÄ Funcionalidades Principais

* **Processamento de PDFs:** Extrai texto de arquivos PDF localizados em uma pasta `data/`.
* **Gera√ß√£o de Embeddings:** Utiliza modelos `SentenceTransformers` para criar representa√ß√µes vetoriais (embeddings) dos trechos de texto.
* **Indexa√ß√£o e Busca Vetorial:** Emprega FAISS para indexar os embeddings e realizar buscas de similaridade eficientes.
* **Integra√ß√£o com Ollama:** Conecta-se a um servidor Ollama local para utilizar diversos LLMs (ex: Llama 3, Mistral) para a gera√ß√£o de respostas.
* **Interface Web Interativa:** Interface de usu√°rio amig√°vel constru√≠da com Streamlit para submeter perguntas e visualizar respostas.
* **Configur√°vel:** Par√¢metros como modelos de embedding e LLM, comportamento de chunking, e depura√ß√£o podem ser ajustados atrav√©s de um arquivo `config.py`.
* **Timestamps no Chat:** Op√ß√£o para exibir data e hora nas mensagens da interface web e do terminal interativo.

## üõ†Ô∏è Tecnologias Utilizadas

* **Python:** Linguagem de programa√ß√£o principal.
* **Ollama:** Para servir LLMs localmente.
* **Streamlit:** Para a interface web.
* **SentenceTransformers:** Para gera√ß√£o de embeddings de texto.
* **FAISS:** Para busca de similaridade em vetores.
* **PyMuPDF (Fitz):** Para extra√ß√£o de texto de PDFs.
* **NumPy:** Para opera√ß√µes num√©ricas.

## ‚öôÔ∏è Configura√ß√£o e Instala√ß√£o

Siga estes passos para configurar e executar o projeto localmente.

### Pr√©-requisitos

1.  **Python:** Vers√£o 3.10 ou 3.11 recomendada.
2.  **Git:** Para clonar o reposit√≥rio (se aplic√°vel) e gerenciar o c√≥digo.
3.  **Ollama:** Servidor Ollama instalado e em execu√ß√£o.
    * Instale a partir de [ollama.com](https://ollama.com/).
    * Baixe o modelo LLM que voc√™ pretende usar (ver `config.py` para o padr√£o). Exemplo:
        ```bash
        ollama pull llama3:latest
        ```
        Verifique os modelos baixados com `ollama list`.

### Passos de Instala√ß√£o

1.  **Clone o Reposit√≥rio (se estiver acessando via Git):**
    ```bash
    git clone <URL_DO_SEU_REPOSITORIO_AQUI>
    cd <NOME_DA_PASTA_DO_REPOSITORIO>
    ```
    Se voc√™ j√° tem os arquivos localmente, pule este passo e navegue at√© a pasta do projeto.

2.  **Crie e Ative um Ambiente Virtual:**
    √â altamente recomendado usar um ambiente virtual.
    ```bash
    python -m venv .venv
    # Linux/macOS
    source .venv/bin/activate
    # Windows
    # .venv\Scripts\activate
    ```

3.  **Instale as Depend√™ncias:**
    Com o ambiente virtual ativo, instale os pacotes listados em `requirements.txt`:
    ```bash
    pip install -r requirements.txt
    ```

### Configura√ß√£o do Projeto

1.  **Arquivo `config.py`:**
    Este arquivo centraliza as configura√ß√µes principais do sistema. Ajuste os seguintes par√¢metros conforme necess√°rio:
    * `PRINT_DEBUG_CHUNKS`: `True` ou `False` para imprimir os chunks recuperados no console.
    * `SHOW_CHAT_TIMESTAMPS`: `True` ou `False` para exibir data/hora no chat.
    * `DEFAULT_OLLAMA_MODEL`: Nome do modelo LLM a ser usado pelo Ollama (ex: `"llama3:70b"`, `"mistral:latest"`).
    * `DEFAULT_EMBEDDING_MODEL`: Nome do modelo SentenceTransformer (ex: `"all-mpnet-base-v2"`).
    * `DEFAULT_DATA_FOLDER`: Nome da pasta para os PDFs (padr√£o: `"data"`).
    * `DEFAULT_CHUNK_SIZE`, `DEFAULT_CHUNK_OVERLAP`: Par√¢metros para divis√£o do texto.
    * `DEFAULT_RETRIEVAL_K`: N√∫mero de chunks a serem recuperados.

2.  **Dados de Entrada (PDFs):**
    * Crie uma pasta chamada `data` (ou o nome definido em `DEFAULT_DATA_FOLDER` no `config.py`) na raiz do projeto.
    * Coloque todos os arquivos PDF que voc√™ deseja que o sistema processe dentro desta pasta.
    * **Nota:** O sistema funciona melhor com PDFs que cont√™m texto selecion√°vel. Arquivos PDF baseados em imagem (scans) sem uma camada de OCR n√£o fornecer√£o texto.

## ‚ñ∂Ô∏è Executando a Aplica√ß√£o

1.  Certifique-se de que o servidor Ollama est√° em execu√ß√£o.
2.  Com seu ambiente virtual ativo e na pasta raiz do projeto, execute:
    ```bash
    streamlit run rag_web.py
    ```
3.  A aplica√ß√£o web ser√° aberta automaticamente no seu navegador padr√£o (geralmente em `http://localhost:8501`).
    * Na primeira execu√ß√£o, pode levar algum tempo para o modelo de embedding ser baixado e os documentos serem processados.

## üìñ Como Usar

1.  Ap√≥s a inicializa√ß√£o, a interface web estar√° pronta.
2.  Na barra lateral, voc√™ pode ver informa√ß√µes sobre o sistema, como os modelos em uso e os arquivos PDF processados.
3.  Digite sua pergunta na caixa de chat na parte inferior da p√°gina e pressione Enter.
4.  O sistema buscar√° informa√ß√µes relevantes nos seus PDFs e usar√° o LLM para gerar uma resposta.
5.  Opcionalmente, ative `PRINT_DEBUG_CHUNKS = True` em `config.py` e monitore o console (onde voc√™ executou `streamlit run ...`) para ver os chunks de texto que foram recuperados para sua pergunta, o que √© √∫til para depura√ß√£o e otimiza√ß√£o.

## üí° Poss√≠veis Melhorias (TODO)

* Implementar estrat√©gias de chunking mais avan√ßadas (ex: sem√¢ntica, baseada em layout).
* Adicionar suporte para outros formatos de documento al√©m de PDF.
* Op√ß√£o para salvar/carregar o √≠ndice FAISS para evitar reprocessamento.
* Interface para gerenciar/selecionar modelos Ollama e de embedding diretamente na UI.
* Mecanismo de feedback do usu√°rio sobre a qualidade das respostas.

## üìÑ Licen√ßa

A definir.