# Sistema de Pesquisa RAG com Ollama, Streamlit e ChromaDB

**Nota Importante:** O c√≥digo-fonte base para o sistema descrito neste guia foi gerado com o valioso apoio da intelig√™ncia artificial Gemini, desenvolvida pelo Google. As funcionalidades e estruturas foram ent√£o iterativamente refinadas e adaptadas para os requisitos espec√≠ficos deste projeto de pesquisa.

## Vis√£o Geral do Projeto

Este projeto implementa um sistema de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) que permite aos usu√°rios fazer perguntas em linguagem natural sobre um conjunto de documentos PDF. O sistema utiliza **ChromaDB** para **armazenamento persistente** de chunks de texto e seus embeddings, resultando em inicializa√ß√µes significativamente mais r√°pidas ap√≥s o primeiro processamento. As respostas s√£o geradas por um Modelo de Linguagem Grande (LLM) hospedado localmente via Ollama, com o contexto relevante extra√≠do dos documentos. A interface do usu√°rio √© fornecida atrav√©s de uma aplica√ß√£o web Streamlit.

O objetivo √© fornecer uma ferramenta de pesquisa sem√¢ntica poderosa e flex√≠vel que possa ser executada localmente, garantindo a privacidade dos dados e permitindo a customiza√ß√£o dos modelos utilizados.

## üöÄ Funcionalidades Principais

* **Processamento de PDFs:** Extrai texto de arquivos PDF localizados em uma pasta `data/`.
* **Persist√™ncia com ChromaDB:** Chunks de texto e seus embeddings s√£o armazenados no ChromaDB, evitando reprocessamento em cada inicializa√ß√£o.
* **Processamento Inteligente:** Verifica arquivos PDF novos ou modificados (com base na data de modifica√ß√£o e tamanho) e atualiza o banco de dados incrementalmente. Arquivos PDF removidos da pasta de dados t√™m seus respectivos chunks deletados do ChromaDB.
* **Gera√ß√£o de Embeddings:** Utiliza modelos `SentenceTransformers` para criar representa√ß√µes vetoriais (embeddings) dos trechos de texto.
* **Busca Vetorial Eficiente:** ChromaDB gerencia a indexa√ß√£o e busca de similaridade.
* **Integra√ß√£o com Ollama:** Conecta-se a um servidor Ollama local para utilizar diversos LLMs (ex: Llama 3, Mistral) para a gera√ß√£o de respostas.
* **Interface Web Interativa:** Interface de usu√°rio amig√°vel constru√≠da com Streamlit para submeter perguntas e visualizar respostas.
* **Interface de Terminal (Opcional):** Um script `rag_terminal.py` para intera√ß√£o via linha de comando tamb√©m est√° dispon√≠vel.
* **Consultas em Lote:** Script `rag_batch_query.py` para processar m√∫ltiplas perguntas de um arquivo texto.
* **Configur√°vel:** Par√¢metros como modelos de embedding e LLM, caminhos de armazenamento, comportamento de chunking, e flags de depura√ß√£o podem ser ajustados centralmente atrav√©s de um arquivo `config.py`.
* **Timestamps no Chat:** Op√ß√£o para exibir data e hora nas mensagens das interfaces de chat.

## üõ†Ô∏è Tecnologias Utilizadas

* **Python:** Linguagem de programa√ß√£o principal (vers√£o 3.10 ou 3.11 recomendada).
* **Ollama:** Para servir LLMs localmente.
* **Streamlit:** Para a interface web.
* **ChromaDB:** Para armazenamento persistente e busca de embeddings e documentos.
* **SentenceTransformers:** Para gera√ß√£o de embeddings de texto.
* **PyMuPDF (Fitz):** Para extra√ß√£o de texto de PDFs.
* **NumPy:** Para opera√ß√µes num√©ricas.

## üèóÔ∏è Arquitetura do Sistema

O diagrama abaixo ilustra os principais componentes do sistema RAG, suas intera√ß√µes e o fluxo de dados, desde o processamento inicial dos documentos PDF at√© a gera√ß√£o da resposta para a consulta do usu√°rio. Ele destaca como as entradas s√£o processadas, onde os dados s√£o armazenados (ChromaDB), e como os diferentes scripts e modelos interagem.

![Diagrama do Sistema RAG](assets/diagrama_rag_sistema.svg)

## üìÇ Estrutura do Projeto

A estrutura de diret√≥rios e arquivos esperada para o projeto √©: