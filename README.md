# Sistema de Pesquisa RAG com Ollama, Streamlit e ChromaDB

Este projeto implementa um sistema de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) que permite aos usu√°rios fazer perguntas em linguagem natural sobre um conjunto de documentos PDF. O sistema agora utiliza **ChromaDB** para **armazenamento persistente** de chunks de texto e seus embeddings, resultando em inicializa√ß√µes significativamente mais r√°pidas ap√≥s o primeiro processamento. As respostas s√£o geradas por um Modelo de Linguagem Grande (LLM) hospedado localmente via Ollama, com o contexto relevante extra√≠do dos documentos. A interface do usu√°rio √© constru√≠da com Streamlit.

**Nota Importante:** O c√≥digo-fonte base para este sistema foi desenvolvido com o valioso apoio da intelig√™ncia artificial Gemini, desenvolvida pelo Google. As funcionalidades e estruturas foram ent√£o iterativamente refinadas e adaptadas para os requisitos espec√≠ficos deste projeto de pesquisa.

## üöÄ Funcionalidades Principais

* **Processamento de PDFs:** Extrai texto de arquivos PDF localizados em uma pasta `data/`.
* **Persist√™ncia com ChromaDB:** Chunks de texto e seus embeddings s√£o armazenados no ChromaDB, evitando reprocessamento em cada inicializa√ß√£o.
* **Processamento Inteligente:** Verifica arquivos PDF novos ou modificados e atualiza o banco de dados incrementalmente.
* **Gera√ß√£o de Embeddings:** Utiliza modelos `SentenceTransformers` para criar representa√ß√µes vetoriais dos trechos de texto.
* **Busca Vetorial Eficiente:** ChromaDB gerencia a indexa√ß√£o e busca de similaridade.
* **Integra√ß√£o com Ollama:** Conecta-se a um servidor Ollama local para utilizar LLMs para a gera√ß√£o de respostas.
* **Interface Web Interativa:** Interface amig√°vel constru√≠da com Streamlit.
* **Configur√°vel:** Par√¢metros como modelos, caminhos, comportamento de chunking e depura√ß√£o s√£o ajustados via `config.py`.
* **Timestamps no Chat:** Op√ß√£o para exibir data e hora nas mensagens.

## üõ†Ô∏è Tecnologias Utilizadas

* **Python:** Linguagem de programa√ß√£o principal.
* **Ollama:** Para servir LLMs localmente.
* **Streamlit:** Para a interface web.
* **ChromaDB:** Para armazenamento persistente e busca de embeddings e documentos.
* **SentenceTransformers:** Para gera√ß√£o de embeddings de texto.
* **PyMuPDF (Fitz):** Para extra√ß√£o de texto de PDFs.
* **NumPy:** Para opera√ß√µes num√©ricas.

## ‚öôÔ∏è Configura√ß√£o e Instala√ß√£o

Siga estes passos para configurar e executar o projeto localmente.

### Pr√©-requisitos

1.  **Python:** Vers√£o 3.10 ou 3.11 recomendada.
2.  **Git:** Para clonar o reposit√≥rio.
3.  **Ollama:** Servidor Ollama instalado e em execu√ß√£o.
    * Instale a partir de [ollama.com](https://ollama.com/).
    * Baixe o modelo LLM desejado (ver `config.py`). Exemplo: `ollama pull llama3:latest`.

### Passos de Instala√ß√£o

1.  **Clone o Reposit√≥rio (se aplic√°vel):**
    ```bash
    git clone <URL_DO_SEU_REPOSITORIO_AQUI>
    cd <NOME_DA_PASTA_DO_REPOSITORIO>
    ```

2.  **Crie e Ative um Ambiente Virtual:**
    ```bash
    python -m venv .venv
    # Linux/macOS: source .venv/bin/activate
    # Windows: .venv\Scripts\activate
    ```

3.  **Instale as Depend√™ncias:**
    O arquivo `requirements.txt` deve conter:
    ```txt
    pymupdf
    sentence-transformers
    ollama
    numpy
    streamlit
    chromadb
    ```
    Com o ambiente virtual ativo, instale:
    ```bash
    pip install -r requirements.txt
    ```

### Configura√ß√£o do Projeto

1.  **Arquivo `config.py`:**
    Ajuste os par√¢metros neste arquivo conforme necess√°rio:
    * `PRINT_DEBUG_CHUNKS`, `SHOW_CHAT_TIMESTAMPS`
    * `DEFAULT_OLLAMA_MODEL`, `DEFAULT_EMBEDDING_MODEL`
    * `DEFAULT_DATA_FOLDER` (padr√£o: `"data"`)
    * `CHROMA_DB_PATH` (padr√£o: `"./chroma_db_store"`): Local onde o banco de dados vetorial ser√° salvo.
    * `CHROMA_COLLECTION_NAME` (padr√£o: `"rag_documents"`)
    * `PROCESSED_FILES_STATUS_JSON` (padr√£o: `"processed_files_status.json"`): Arquivo que rastreia o estado dos PDFs.
    * `DEFAULT_CHUNK_SIZE`, `DEFAULT_CHUNK_OVERLAP`, `DEFAULT_RETRIEVAL_K`.

2.  **Dados de Entrada (PDFs):**
    * Crie uma pasta chamada `data` (ou o nome definido em `DEFAULT_DATA_FOLDER`) na raiz do projeto.
    * Coloque seus arquivos PDF dentro desta pasta.

## ‚ñ∂Ô∏è Executando a Aplica√ß√£o

1.  Certifique-se de que o servidor Ollama est√° em execu√ß√£o.
2.  Com seu ambiente virtual ativo e na pasta raiz do projeto, execute:
    ```bash
    streamlit run rag_web.py
    ```
3.  Acesse a interface em seu navegador (geralmente `http://localhost:8501`).
    * **Primeira Execu√ß√£o:** Levar√° mais tempo, pois os PDFs ser√£o processados, e os dados ser√£o salvos no ChromaDB (na pasta definida por `CHROMA_DB_PATH`).
    * **Execu√ß√µes Subsequentes:** Ser√£o significativamente mais r√°pidas, pois os dados processados ser√£o carregados do ChromaDB. Apenas PDFs novos ou modificados na pasta `data/` ser√£o reprocessados.

## üìñ Como Usar

1.  Ap√≥s a inicializa√ß√£o, interaja com o chat na interface web.
2.  Consulte a barra lateral para informa√ß√µes sobre os modelos e arquivos processados.
3.  Para depura√ß√£o da relev√¢ncia dos chunks, ative `PRINT_DEBUG_CHUNKS = True` em `config.py` e observe o console.

## üí° Poss√≠veis Melhorias (TODO)

* Implementar estrat√©gias de chunking mais avan√ßadas.
* Adicionar suporte para outros formatos de documento.
* Interface para gerenciar modelos e reindexa√ß√£o de dados.
* Mecanismo de feedback do usu√°rio.

## üìÑ Licen√ßa

A definir.